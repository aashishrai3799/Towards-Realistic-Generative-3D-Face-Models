# '''
# first step:
# pre-train the coarse model (i.e.ğ¸ğ‘) for two epochs with a batch size of 64,
# with ğœ†ğ‘™ğ‘šğ‘˜=1ğ‘’âˆ’4, ğœ†ğ‘’ğ‘¦ğ‘’=1.0, ğœ†ğœ·=1ğ‘’âˆ’4, and ğœ†ğ=1ğ‘’âˆ’4

# Why:
# training with only lmk loss for good initialization, 
# because the use of photometric loss needs good initialization both in regression and optimization
# and also, photometric loss needs differentiable rendering that makes the training slow
# 
# 
# '''
output_dir: "./training_logs"
pretrained_modelpath: './data/mica_combined_ft0_100k.tar'
dataset:
  # training_data: ['vggface2', 'vox2']
  training_data: ['celebahq']
  eval_data: ['now']
  batch_size: 1
  K: 1
loss:
  photo: 0.1
  id: 0.
  useSeg: False
  reg_tex: 0.
  reg_light: 0.
  shape_consistency: False
  lmk: 5.0
train:
  resume: True
  max_epochs: 500000
  max_steps: 100000
  log_steps: 10
  vis_steps: 500
  checkpoint_steps: 1000
  val_steps: 500
  eval_steps: 1000
use_mica: True  # Uncomment this to run MICA training
